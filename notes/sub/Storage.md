**Types**:: #ðŸŒ²  
**Tags**:: [[Kubernetes]], [[K8SPresentation]], [[Server]]

---

%%
[Volumes | Kubernetes](https://kubernetes.io/docs/concepts/storage/volumes/#volume-types)
[Storage Classes | Kubernetes](https://kubernetes.io/docs/concepts/storage/storage-classes/)
[kubernetes - What is the difference between persistent volume (PV) and persistent volume claim (PVC) in simple terms? - Stack Overflow](https://stackoverflow.com/questions/48956049/what-is-the-difference-between-persistent-volume-pv-and-persistent-volume-clai)
%%

> [!warning]  
> Je n'expose ici que les concepts gÃ©nÃ©raux, le stockage Ã©tant une chose sensible et complexe sous kubernetes. Je vous recommande de chercher les solutions de stockage qui vous conviennent le mieux

# Intro

Comme vu prÃ©cÃ©demment on peut monter des volumes venant de myriades de sources diffÃ©rentes, le vaste choix est ainsi permis par les Storages Types.
Dans notre cas d'auto-hÃ©bergement + baremetal, les seuls providers qui nous intÃ©ressent vraiment sont ceux qui ne concernent pas les clouds providers, ce qui nous amÃ¨ne ainsi dans le cÅ“ur du sujet. **Les solutions de stockages pour K8S**

# Volume a.k.a Traditional Storage

> _On parle ici juste des `Types of volumes` mis dans la template de pods_

Pratique de prime abord pratique, mais limitÃ©e, car trÃ¨s _manuelle_ et force la configuration du stockage dans chaque template -> liÃ© au fonctionnement du pod  
=> pas foule de solutions _pratiques_ non plus

## NFS

> [!info]
>
> - [nfs-server-provisioner 1.3.0 Â· raphael/raphael](https://artifacthub.io/packages/helm/raphael/nfs-server-provisioner)
> - [Using persistent NFS volumes in ReadWriteMany for k8s](https://www.padok.fr/en/blog/readwritemany-nfs-kubernetes)

On doit avoir un serveur NFS et configurer les volumes Ã  partager en amont  
=> pas automatisÃ© et gÃ©rÃ© par K8S -> pas pratique

Des solutions existent pour automatiser son dÃ©ploiement, mais on ne va pas les Ã©voque rdans ce tutoriel pour diverses raisons (support communautaire, tendance Ã  abandonner le NFS dans les divers projets K8S \#[rook](https://github.com/rook/nfs/tree/6bee1a7852318c6b67828ee982604f724b5a7088#nfs))

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: foo
spec:
  containers: ...
  volumes:
    - name: test-volume
      nfs:
        server: my-nfs-server.example.com
        path: /my-nfs-volume
        readOnly: true
```

## HostPath

> [!danger]  
> Les volumes HostPath prÃ©sentent de nombreux risques pour la sÃ©curitÃ© et il est prÃ©fÃ©rable d'Ã©viter leur utilisation dans la mesure du possible. Lorsqu'un volume HostPath doit Ãªtre utilisÃ©, il doit Ãªtre limitÃ© au fichier ou au rÃ©pertoire requis et montÃ© en lecture seule.  
> => parfois nÃ©cessaire pour certains type d'application

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: bar
spec:
  containers:
    ...
  volumes:
	hostPath:
      path: /data # Le chemin du dossier sur le node
      type: Directory # Type de montage (opt)
```

# PV & PVC a.k.a Cloud Native Storage

ConÃ§ut pour s'intÃ©grer dans Kubernetes -> intÃ¨gre la mÃ©canique de PV et de PVC

> [!todo]
> reprendre la ph

Le PV permet de fournir le stockage Ã  PVC, celui-ci est dÃ©jÃ  disponible / configurÃ© manuellement -> permet de dÃ©coupler les fonctions de stockage du pod. Il contient tous les dÃ©tails de la backend de stockage

Le PVC fait office de requÃªte de stockage  
=> dit qu'un PV va Ãªtre crÃ©Ã© + dit Ã  Kubernetes quoi crÃ©er / Ã  qui l'associer ensuiteâ€¦

---

Avec cette mÃ©canique, pour fournir un volume de 10go Ã  un pod il faut :

1. crÃ©er un PVC + PV **manuellement et correspondant l'un Ã  l'autre**
2. fournir le PVC au [[Pods|Pod]]

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local

spec:
  storageClassName: manual # L'identifiant
  capacity:
    storage: 10Gi # Quoi fournir
  accessModes: # La faÃ§on de monter le volume => Read Only...
    - ReadWriteOnce
  hostPath: # La config du volume comme au dÃ©part
    path: "/mnt/data"
```

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: foobar-storage
spec:
  storageClassName: manual # L'identifiant Ã  utiliser
  accessModes: # La faÃ§on de monter le volume => Read Only...
    - ReadWriteOnce
  resources: # Quoi fournir
    requests:
      storage: 2.5Gi
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foobar
spec:
  replicas: 1
  selector:
    matchLabels:
      app: foobar
  template:
    metadata:
      labels:
        app: foobar
    spec:
      containers:
        - name: foobar
          image: gcr.io/kuar-demo/kuard-amd64:blue
          volumeMounts:
            - name: quz-volume
              mountPath: /etc/baz/
          ...
      volumes:
        - name: quz-volume
          persistentVolumeClaim:
            claimName: foobar-storage
```

## Rook

> [!info]
>
> - [Rook](https://rook.io/)

Une solution qui utilise (principalement) CEPH en arriÃ¨re-plan  
=> CNCF Graduated âœ…

---

> [!warning]  
> MalgrÃ© son aspect pratique rook semble cependant subir certains problÃ¨mes suivant mes lectures, de plus il semble aussi manquer de snapshot des volumes sans passer par des outils externes \#[[K8S Admin Apps]]

![Rook's Storage Architecture](https://rook.io/docs/rook/v1.11/Getting-Started/ceph-storage/kubernetes.png)

## Longhorn

> [!info]
>
> - [Longhorn](https://longhorn.io/)

> [!warning] Disclaimer  
> Je parle beaucoup de cette solution, car c'est celle que j'ai le plus utilisÃ©e

Une applications qui utilise une solution de stockage maison  
=> CNCF Incubating âœ…

---

Longhorn possÃ¨de de nombreux avantages Ã  ne pas utiliser de sous systÃ¨me de stockage comme [[#Rook]] avec [[Ceph]]

1. Stockage persistant hautement disponible pour Kubernetes -> utilise directement le stockage disponible sur les nÅ“uds pour le fournir Ã  Kubernetes, que ce soit un disque, une partition d'un disqueâ€¦ Il se dÃ©marque ainsi de [[#Rook]] qui lui doit avoir [des disques vierges](https://rook.io/docs/rook/v1.11/Getting-Started/Prerequisites/prerequisites/#ceph-prerequisites) Ã  cause de [[Ceph]]  
   => pratique++
2. Sauvegardes / snapshot instantanÃ©es incrÃ©mentielles faciles
3. Disaster recovery intÃ©grÃ©e
   > [!warning]  
   > NÃ©cessite un serveur NFS / S3

> [!warning]  
> PrÃ©venir des petits PB perso avec cette solutions

![Longhorn's storage architecture](https://longhorn.io/img/diagrams/architecture/how-longhorn-works.svg)

# Storage Class

Vous devez surement vous demander en quoi les [[#Cloud Native Storage]] sont si cloud native, la rÃ©ponse est simple. Ceux-ci ne s'utilisent pas comme nous l'avons fait depuis le dÃ©but avec les [[#Traditional Storage]], ils utilisent le mÃ©canisme des _Storage Classes_.

Les _Storage Classes_ sont un moyen de dÃ©finir un type de stockage dans un cluster Kubernetes.

Ainsi, elles permettent le _dynamique provisionning_ avec l'aide des PVC. Cela nous permet donc d'Ã©viter de devoir fournir Ã  chaque fois un volume prÃ©configurÃ© manuellement Ã  notre application et de juste indiquer Ã  Kubernetes que faire, le rÃªve ðŸ¤¤

%%  
[D2 Playground](https://play.d2lang.com/?script=3FNNS8UwELzvr5g_oDzbCpJr7yr4eHiNbSiPFxMJrSLS_y6btNHWrR9Xb8nMZpbMzubxKrT-xRFNIVB4I-DB-uaUjkCsVyh2OwJGAlhwRV5lUg-9j-ETnwPxmzPHV7ZBoQvGuAV2Nj-M6DjJP2o3aPs37WDaXykPTtsb18h_21T3QbvO_NhgJNrzzrHn2uLA4TbYvz5NkjneChUR0HC2j86Ei_M4GpWG8pkpNplyk6mWDAG3hzq3SO4msJDAUgKrFRhRSVOSlBQFwbveNyfdmZnhDLKntfVDi2vdH58NFwXdGdznU52qVwZf_iuD88YtPV7CpQxXX-FkNrsXzftoPa_HN-N4DwAA__8%3D&)  
%%

```d2
direction: down

classes: {
  block: {
    width: 200
  }
  line: {
    width: 800
  }
  automated: {
    width: 200
    style: {
      stroke: green
      stroke-width: 2
    }
  }
  manual: {
    width: 200
    style: {
      stroke: red
      stroke-width: 2
    }
  }
  maunalOnce: {
    width: 800
    style: {
      stroke: orange
      stroke-width: 2
    }
  }
}

Traditional Volume Type: {
  grid-rows: 4

  container1.class: block
  container2.class: block
  container3.class: block
  container4.class: block

  PVC1.class: manual
  PVC2.class: manual
  PVC3.class: manual
  PVC4.class: manual

  PV1.class: manual
  PV2.class: manual
  PV3.class: manual
  PV4.class: manual

  Stockage.class: line
}

Cloud Native Storage X Storage Class: {
  grid-rows: 5

  container1.class: block
  container2.class: block
  container3.class: block
  container4.class: block

  PVC1.class: manual
  PVC2.class: manual
  PVC3.class: manual
  PVC4.class: manual

  PV1.class: automated
  PV2.class: automated
  PV3.class: automated
  PV4.class: automated

  StorageClass.class: maunalOnce

  Stockage.class: line
}
```

---

> [!note]  
> Example StorageClass + PVC

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: longhorn-fast-durable # Le nom de la classe
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Retain # La facons de gÃ©rer les PVs apprÃ¨s que les PVC soit delete
parameters:
  numberOfReplicas: "1" # Nombre de rÃ©plicas
  staleReplicaTimeout: "2880" # 48 hours in minutes
  fromBackup: ""
  fsType: ext4
  diskSelector: "nvme"
```

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: foobar-storage
spec:
  storageClassName: longhorn-fast-durable # La storage class Ã  utiliser
  accessModes: # La facons de monter le volume => Read Only...
    - ReadWriteOnce
  resources: # Quoi fournir
    requests:
      storage: 2.5Gi
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foobar
spec:
  replicas: 1
  selector:
    matchLabels:
      app: foobar
  template:
    metadata:
      labels:
        app: foobar
    spec:
      containers:
        - name: foobar
          image: gcr.io/kuar-demo/kuard-amd64:blue
          volumeMounts:
            - name: quz-volume
              mountPath: /etc/baz/
          ...
      volumes:
        - name: quz-volume
          persistentVolumeClaim:
            claimName: foobar-storage
```
